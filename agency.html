<!doctype html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="">
    <meta name="author" content="">
    <link rel="icon" href="favicon.ico">

    <title>MOCORE | Research</title>
    <!-- Fonts -->
    <link href="https://fonts.googleapis.com/css?family=Oswald:300,400,500|Roboto:100,300,400,700" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Poppins&display=swap" rel="stylesheet">
    <link href="assets/css/bootstrap.css" rel="stylesheet">
    <link href="assets/css/header.css" rel="stylesheet">

</head>

<body>
    <nav class="navbar navbar-expand-md navbar-dark fixed-top" name="top">
        <a class="navbar-brand" href="index.html"><img src="" height="30px;"></a>
        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarsExampleDefault" aria-controls="navbarsExampleDefault" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
        </button>
        <div class="navbar-collapse collapse" id="navbarsExampleDefault">
            <ul class="navbar-nav mr-auto">
                <li class="nav-item">
                    <a class="nav-link" href="/index.html">Home</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" href="/research.html">Research</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" href="/people.html">People</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" href="/events.html">Events</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" href="/publications.html">Publications</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" href="/presentations.html">Presentations</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link nav-link-end" href="/contact.html">Contact</a>
                </li>
            </ul>
        </div>
    </nav>
    <main role="main">
        <div class="header_agency header_sub">
            <div class="container">
                <div class="row">
                    <div class="offset-md-2 col-md-8 header_text">
                        <h1>Leverage Cognitive Factors for Rehabilitation and Movement Training</h1>
                    </div>
                </div>
            </div>
        </div>

        <section id="section_01" style="text-align: center;">
            <div class="container">
                <div class="row">
                    <div class="col-md-12">
                        <h4>Our objective is to investigate the potential role of cognitive factors</br>
                        in movement training and leverage them to optimize upper-extremity rehabilitation</h4>
                    </div>
                </div>
            </div>
        </section>

        <section id="section_01">
            <div class="container">
                <div class="row collaborators">
                    <div class="col-md-1">
                        <hr class="section_divider">
                    </div>
                    <div class="col-md-11">
                        <h3 class="subpage_header">Role of Agency in Hand Reach and Grasp Rehabilitation</h3>
                        <hr class="section_divider">
                    </div>
                </div>
                <div class="row collaborators">
                    <div class="col-md-12">
                        <p>
                            The goal of this project is to investigate the role of cognitive factors, which pomotes neural engagement for computerized interfaces, in hand reach and grasp tasks for neurotrauma rehabilitation purposes. In this
                            project, we aim to explicitly monitor and leverage cognitive factors towards better movement performance. The results will be clinical relevant for hand prostheses training after SCI or upper-limb amputation and providing
                            better virtual reality environments to accelerate post-stroke motor-relearning.
                        </p></br>
                        <div class="col-md-12" style="text-align: center">
                            <h3><b>Reach Pinch Task</b></h3></br></br>
                        </div>
                    </div>
                    <section id="section_01" style="text-align: center; margin:0;">
                        <div class="container">
                            <div class="row">
                                <div class="col-md-6">
                                    <video width="100%" controls><source src="/assets/videos/agency/Reach_Pinch_Protocol_1.mp4" type="video/mp4"></video>
                                </div>
                                <div class="col-md-6">
                                    <video width="100%" controls><source src="/assets/videos/agency/Reach_Pinch_Protocol_2.mp4" type="video/mp4"></video>
                                </div>
                            </div>
                        </div>
                    </section>
                    <div class="col-md-12" style="text-align: center">
                        </br></br>
                        <h3><b>Hand Grasp Task</b></h3></br></br>
                    </div>
                    <section id="section_01" style="text-align: center; margin:0;">
                        <div class="container">
                            <div class="row">
                                <div class="col-md-6">
                                    <video width="100%" controls><source src="/assets/videos/agency/Hand_Grasp_Protocol_1.mp4" type="video/mp4"></video>
                                </div>
                                <div class="col-md-6">
                                    <video width="100%" controls><source src="/assets/videos/agency/Hand_Grasp_Protocol_2.mp4" type="video/mp4"></video>
                                </div>
                            </div>
                        </div>
                    </section>
                </div>
            </div>
        </section>

        <section id="section_01">
            <div class="container">
                <div class="row collaborators">
                    <div class="col-md-1">
                        <hr class="section_divider">
                    </div>
                    <div class="col-md-11">
                        <h3 class="subpage_header">Virtual Reality Hand-Grasp Combined With Electroencephalography</h3>
                        <hr class="section_divider">
                    </div>
                </div>
                <div class="row collaborators">
                    <div class="col-md-12">
                        <p>
                            In this project, we aim to investigate the relationship between sense of agency and electroencephalography (EEG) signals. The subjects performed hand-grasp motion in a virtual reality environment while EEG signals were
                            measured from different brain regions associated with agency and readiness potential [1]. Our results indicate a positive R-relationship between agency and increased beta band apparent. With further analysis we aim to
                            develop EEG-based automated procedure for adapting prosthesis control with better sense of agency and movement control.
                        </p>
                    </div>
                </div>
                </br>
                <section id="section_01" style="text-align: center; margin:0;">
                    <div class="container">
                        <div class="row">
                            <div class="col-md-6">
                                <img src="/assets/images/research/agency/eeg4.jpg" style="border-style: solid; border-width: 2px;"></img></br></br></br>
                            </div>
                            <div class="col-md-6">
                                <video width="100%" controls><source src="/assets/videos/agency/VR_Grasp_Protocol.mp4" type="video/mp4"></video>
                            </div>
                        </div>
                    </div>
                </section>
            </div>
        </section>

        <section id="section_01">
            <div class="container">
                <div class="row collaborators">
                    <div class="col-md-1">
                        <hr class="section_divider">
                    </div>
                    <div class="col-md-11">
                        <h3 class="subpage_header">Virtual Reality Hand-Reach With Reward Feedback</h3>
                        <hr class="section_divider">
                    </div>
                </div>
                <div class="row collaborators">
                    <div class="col-md-8">
                        <p>
                            In this project, we aim to study the relationship between the sense of agency and visual feedback. Specifically, the visual feedback as a bi-modal knowledge of reward and penalty based on motion performance. The study was
                            performed with a hand-reach task in a virtual reality environment. Motion performance metrics included speed, accuracy and path-length. Our preliminary results show that the 'reward feedback' improves sense of agency and
                            reduces beta activation
                        </p>
                    </div>
                    <div class="col-md-4">
                        <img src="/assets/images/research/agency/vrhandreach.jpg" style="border-style: solid; border-width: 2px;"></img></br></br></br>
                    </div>
                </div>
                <section id="section_01" style="text-align: center; margin:0;">
                    <div class="container">
                        <div class="row">
                            <div class="col-md-6">
                                <video width="100%" controls><source src="/assets/videos/agency/Rewards_Protocol_Positive_Feedback.mp4" type="video/mp4"></video>
                            </div>
                            <div class="col-md-6">
                                <video width="100%" controls><source src="/assets/videos/agency/Rewards_Protocol_Negative_Feedback.mp4" type="video/mp4"></video>
                            </div>
                        </div>
                    </div>
                </section>
            </div>
        </section>

        <section id="section_01">
            <div class="container">
                <div class="row collaborators">
                    <div class="col-md-1">
                        <hr class="section_divider">
                    </div>
                    <div class="col-md-11">
                        <h3 class="subpage_header">Cognition Glove</h3>
                        <hr class="section_divider">
                    </div>
                </div>
                <div class="row collaborators">
                    <div class="col-md-12">
                        <p>
                            In this project, we are developing an instrumented glove that effectively predicts secure grasp with force/flex data from the glove, using visual feedback and force measurement of a continuous pinch task. The Cognition
                            Glove is also coupled with grasp task completion to sensory sequences, including visual, audio, and haptic feedback. We aim to reverse-engineer the sense of agency, where we produce a paradigm to modulate motion control
                            performance to achieve a better sense of agency. This correlation between motor performance and sense of agency will be applied for benefiting neuro-motor rehabilitation for post-stroke patients.
                        </p>
                    </div>
                </div>
                <div class="row collaborators">
                    <div class="col-md-6">
                        <img src="/assets/images/research/agency/slide1_orig.jpg" style="border-style: solid; border-width: 2px;"></img></br></br></br>

                    </div>
                    <div class="col-md-6">
                        <img src="/assets/images/research/agency/slide2_orig.jpg" style="border-style: solid; border-width: 2px;"></img></br></br></br>

                    </div>
                </div>
            </div>
        </section>
        <section id="section_01">
            <div class="container">
                <div class="row collaborators">
                    <div class="col-md-1">
                        <hr class="section_divider">
                    </div>
                    <div class="col-md-11">
                        <h3 class="subpage_header">Virtual Reality Agency Tuning With Multi-Degree of Freedom System</h3>
                        <hr class="section_divider">
                    </div>
                </div>
                <div class="row collaborators">
                    <div class="col-md-12">
                        <p>
                            In this project, our goal is to investigate a multi degree-of-freedom (DOF) device's tuning according to sense of agency. We hypothesize that the operation in each DOF could be uniquely tuned according to the user's
                            optimal sense of agency. A device is tuned based upon user-specific optimal parameters and performance is measured in a motion task. We also aim to investigate the point-of-interface difference between a 6-DOF Steward
                            platform (that can physically translate and rotate) to a 6-DOF space mouse. We hypothesize that the Steward platform with additional point-of interface DOF should help user to produce greater proprioceptive involvement
                            and achieve better agency tuning.
                        </p>
                    </div>
                </div>
                </br>
                <section id="section_01" style="text-align: center; margin:0;">
                    <div class="container">
                        <div class="row">
                            <div class="col-md-6">
                                <img src="/assets/images/research/agency/joystick-picture.jpg" style="border-style: solid; border-width: 2px;"></img></br></br></br>
                            </div>
                            <div class="col-md-6">
                                <img src="/assets/images/research/agency/picture2.jpg" style="border-style: solid; border-width: 2px;"></img></br></br></br>
                            </div>
                        </div>
                    </div>
                </section>
            </div>
        </section>

        <section id="section_01">
            <div class="container">
                <div class="row collaborators">
                    <div class="col-md-1">
                        <hr class="section_divider">
                    </div>
                    <div class="col-md-11">
                        <h5 class="subpage_header">References</h5>
                        <hr class="section_divider">
                    </div>
                </div>
                <div class="row collaborators" style="margin-top:50px;">
                    <div class="col-md-12">
                        <p>
                            1. ​H.-G. Jo, M. Wittmann, T. Hinterberger, and S. Schmidt, “The readiness potential reflects intentional binding,” Front. Hum. Neurosci., vol. 8, Jun. 2014.​
                        </p>
                    </div>
                </div>
            </div>
        </section>

    </main>

    <section class="vin_footer">
        <footer class="container">
            <div class="row">
                <div class="col-md-4">
                    <p>© 2019 MOCORE Laboratory</p>
                </div>
                <div class="col-md-8" style="text-align:right">
                    <p style="padding-top:15px"><a class="btn btn-primary btn-lg card_btn" href="#top" role="button">Back to top</a></p>
                </div>
            </div>
        </footer>
    </section>

    <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
    <script src="assets/js/bootstrap.min.js"></script>
</body>

</html>
